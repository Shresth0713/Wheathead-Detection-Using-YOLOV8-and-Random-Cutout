{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39229679",
   "metadata": {},
   "source": [
    "## Random cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177ec09a-6028-4bcb-828c-d4f3e3b2bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the directory where your dataset images and annotations are stored\n",
    "dataset_dir = 'F:/Minor 5/arvalis_1/image'\n",
    "output_dir = 'F:/Minor 5/arvalis_1/masked'\n",
    "annotations_dir = 'F:/Minor 5/arvalis_1/label'  # Directory containing YOLO format annotation files\n",
    "\n",
    "# Define the size of the mask and the number of random images you want to process\n",
    "mask_size = (190,40)\n",
    "num_images_to_process = 505\n",
    "#num_masks_per_image = 7\n",
    "max_rotation_angle = 90\n",
    "mask_color = (0, 255, 0)\n",
    "\n",
    "# Function to extract bounding box centers from the YOLO format annotation file\n",
    "def extract_bounding_box_centers(annotation_file_path):\n",
    "    centers = []\n",
    "    with open(annotation_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            label = int(parts[0])  # assuming label is the first element\n",
    "            x_center, y_center, width, height = map(float, parts[1:5])\n",
    "\n",
    "            # Convert YOLO format to bounding box coordinates (xmin, ymin, xmax, ymax)\n",
    "            xmin = int((x_center - width / 2) * image_width)\n",
    "            ymin = int((y_center - height / 2) * image_height)\n",
    "            xmax = int((x_center + width / 2) * image_width)\n",
    "            ymax = int((y_center + height / 2) * image_height)\n",
    "\n",
    "            center_x = (xmin + xmax) // 2\n",
    "            center_y = (ymin + ymax) // 2\n",
    "\n",
    "            centers.append((center_x, center_y))\n",
    "\n",
    "    return centers\n",
    "\n",
    "# Randomly select and process a subset of images\n",
    "selected_images = random.sample(os.listdir(dataset_dir), num_images_to_process)\n",
    "\n",
    "for image_file in selected_images:\n",
    "    image_path = os.path.join(dataset_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    annotation_file_path = os.path.join(annotations_dir, image_file.replace('.JPG', '.txt'))\n",
    "\n",
    "    if os.path.exists(annotation_file_path):\n",
    "        bounding_box_centers = extract_bounding_box_centers(annotation_file_path)\n",
    "        num_masks_per_image = int(len(bounding_box_centers) * 0.6)\n",
    "        #for _ in range(num_masks_per_image):\n",
    "        if bounding_box_centers:\n",
    "                selected_centers = random.sample(bounding_box_centers, min(num_masks_per_image, len(bounding_box_centers)))\n",
    "\n",
    "                modified_image = image.copy()\n",
    "\n",
    "                for center_x, center_y in selected_centers:\n",
    "                    mask = np.zeros_like(image)\n",
    "                    mask_center_x, mask_center_y = mask_size[0] // 2, mask_size[1] // 2\n",
    "                    mask_start_x, mask_start_y = max(center_x - mask_center_x, 0), max(center_y - mask_center_y, 0)\n",
    "                    mask_end_x, mask_end_y = min(center_x + mask_center_x, image_width), min(center_y + mask_center_y, image_height)\n",
    "\n",
    "                    mask[mask_start_y:mask_end_y, mask_start_x:mask_end_x] = mask_color\n",
    "                    rotation_angle = random.uniform(-max_rotation_angle, max_rotation_angle)\n",
    "                    rotation_matrix = cv2.getRotationMatrix2D((center_x, center_y), rotation_angle, 1)\n",
    "                    rotated_mask = cv2.warpAffine(mask, rotation_matrix, (image_width, image_height))\n",
    "\n",
    "            # Apply the rotated mask to the modified image\n",
    "\n",
    "                # Apply the mask to the modified image\n",
    "                    modified_image = cv2.addWeighted(modified_image, 1, rotated_mask, 1, 0)\n",
    "\n",
    "                # Save the modified image\n",
    "                output_file = os.path.join(output_dir, image_file)\n",
    "                cv2.imwrite(output_file, modified_image)\n",
    "        os.remove(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729c8ac",
   "metadata": {},
   "source": [
    "## SPLITTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8cc4e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training images are :  1008\n",
      "Validation images are :  252\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import shutil\n",
    "\n",
    "#arrays to store file names\n",
    "imgs =[]\n",
    "xmls =[]\n",
    "\n",
    "#setup dir names\n",
    "trainPath = 'D:/Minor 5/YOLOV8/Dataset/images/train'\n",
    "valPath = 'D:/Minor 5/YOLOV8/Dataset/images//val'\n",
    "crsPath = 'F:/Minor 5/global/combgreen' #dir where images and annotations stored\n",
    "\n",
    "#setup ratio (val ratio = rest of the files in origin dir after splitting into train and test)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "\n",
    "#total count of imgs\n",
    "totalImgCount = len(os.listdir(crsPath))/2\n",
    "\n",
    "#soring files to corresponding arrays\n",
    "for (dirname, dirs, files) in os.walk(crsPath):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):\n",
    "            xmls.append(filename)\n",
    "        else:\n",
    "            imgs.append(filename)\n",
    "\n",
    "\n",
    "#counting range for cycles\n",
    "countForTrain = int(len(imgs)*train_ratio)\n",
    "countForVal = int(len(imgs)*val_ratio)\n",
    "print(\"training images are : \",countForTrain)\n",
    "print(\"Validation images are : \",countForVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b17ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Minor 5/YOLOV8/Dataset/images//val\\\\combgreen'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimagePath = 'F:/Minor 5/YOLOV8/Dataset/train/images'\n",
    "trainlabelPath = 'F:/Minor 5/YOLOV8/Dataset/train/labels'\n",
    "valimagePath = 'F:/Minor 5/YOLOV8/Dataset/val/images'\n",
    "vallabelPath = 'F:/Minor 5/YOLOV8/Dataset/val/labels'\n",
    "#cycle for train dir\n",
    "for x in range(countForTrain):\n",
    "\n",
    "    fileJpg = choice(imgs) # get name of random image from origin dir\n",
    "    fileXml = fileJpg[:-4] +'.txt' # get name of corresponding annotation file\n",
    "\n",
    "    #move both files into train dir\n",
    "    #shutil.move(os.path.join(crsPath, fileJpg), os.path.join(trainimagePath, fileJpg))\n",
    "    #shutil.move(os.path.join(crsPath, fileXml), os.path.join(trainlabelPath, fileXml))\n",
    "    shutil.copy(os.path.join(crsPath, fileJpg), os.path.join(trainimagePath, fileJpg))\n",
    "    shutil.copy(os.path.join(crsPath, fileXml), os.path.join(trainlabelPath, fileXml))\n",
    "\n",
    "\n",
    "    #remove files from arrays\n",
    "    imgs.remove(fileJpg)\n",
    "    xmls.remove(fileXml)\n",
    "\n",
    "\n",
    "\n",
    "#cycle for test dir   \n",
    "for x in range(countForVal):\n",
    "\n",
    "    fileJpg = choice(imgs) # get name of random image from origin dir\n",
    "    fileXml = fileJpg[:-4] +'.txt' # get name of corresponding annotation file\n",
    "\n",
    "    #move both files into train dir\n",
    "    #shutil.move(os.path.join(crsPath, fileJpg), os.path.join(valimagePath, fileJpg))\n",
    "    #shutil.move(os.path.join(crsPath, fileXml), os.path.join(vallabelPath, fileXml))\n",
    "    shutil.copy(os.path.join(crsPath, fileJpg), os.path.join(valimagePath, fileJpg))\n",
    "    shutil.copy(os.path.join(crsPath, fileXml), os.path.join(vallabelPath, fileXml))\n",
    "    \n",
    "    #remove files from arrays\n",
    "    imgs.remove(fileJpg)\n",
    "    xmls.remove(fileXml)\n",
    "\n",
    "#rest of files will be validation files, so rename origin dir to val dir\n",
    "#os.rename(crsPath, valPath)\n",
    "shutil.move(crsPath, valPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2274d",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = YOLO('yolov8x.pt')\n",
    "model2.train(data=\"dataset5.yaml\", epochs = 80, patience=60,cos_lr= True, dropout= 0.2, plots= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pgu",
   "language": "python",
   "name": "shres"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
